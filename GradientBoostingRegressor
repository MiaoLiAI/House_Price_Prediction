from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt
import numpy as np

# =========================
# 1. Train Gradient Boosting
# =========================
gbr = GradientBoostingRegressor(
    n_estimators=1000,          # Set a large number, use early stopping
    learning_rate=0.05,
    max_depth=3,
    validation_fraction=0.1,    # Use 10% of the training set for early stopping
    n_iter_no_change=20,        # Stop if validation error does not improve for 20 consecutive trees
    tol=1e-4,                   # Threshold for error improvement
    random_state=42
)

gbr.fit(X_train, Y_train)

# =========================
# 2. Compute loss using staged_predict
# =========================
train_loss = []
val_loss = []

for y_train_pred, y_val_pred in zip(
    gbr.staged_predict(X_train),
    gbr.staged_predict(X_test)
):
    train_loss.append(mean_squared_error(Y_train, y_train_pred))
    val_loss.append(mean_squared_error(Y_test, y_val_pred))

# =========================
# 3. Visualization
# =========================
plt.figure(figsize=(8, 5))
plt.semilogy(train_loss, label='Train')
plt.semilogy(val_loss, label='Validation')
plt.xlabel('Number of Trees')
plt.ylabel('MSE (log scale)')
plt.title('Gradient Boosting Training vs Validation Loss')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

resultgbr = cross_validate(
    gbr, X_train, Y_train, 
    cv=cv, scoring='r2', return_train_score=True
)
print(resultgbr['train_score'].mean(), result1['test_score'].mean())
